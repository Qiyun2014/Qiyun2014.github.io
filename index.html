<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta property="og:type" content="website">
<meta property="og:title" content="祁云的博客">
<meta property="og:url" content="https://github.com/Qiyun2014/index.html">
<meta property="og:site_name" content="祁云的博客">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="祁云的博客">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://github.com/Qiyun2014/">





  <title>祁云的博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">祁云的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Qiyun2014/2019/02/15/ImageMagick-7-0-7-16使用/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qiyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="祁云的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/15/ImageMagick-7-0-7-16使用/" itemprop="url">ImageMagick 7.0.7-16使用</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-02-15T15:08:48+08:00">
                2019-02-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="ImageMagick-7-0-7-16使用"><a href="#ImageMagick-7-0-7-16使用" class="headerlink" title="ImageMagick 7.0.7-16使用"></a>ImageMagick 7.0.7-16使用</h2><p><strong>相关链接</strong></p>
<ul>
<li>HomePage: <a href="https://www.imagemagick.org/script/index.php" target="_blank" rel="noopener">https://www.imagemagick.org/script/index.php</a></li>
<li>Download: <a href="https://github.com/ImageMagick/ImageMagick">https://github.com/ImageMagick/ImageMagick</a></li>
</ul>
<p>##安装<br>    •brew install ImageMagick</p>
<p>##使用</p>
<p><strong>Edge Detection</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">convert input.png -colorspace gray -canny 0x1+10%+30% output.png</span><br></pre></td></tr></table></figure></p>
<p><strong>Compositon Image</strong></p>
<p>把一张图片按80的质量去压缩(jpg的压缩参数),同时按图片比例非强制缩放成不超过280x140的图片.居中裁剪280x140,去掉图片裁减后的空白和图片exif信息,通常这种指令是为了保证图片大小正好为280x140<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">convert +profile &apos;*&apos; /Users/qiyun/Desktop/屏幕快照\ 2018-01-03\ 下午2.04.04.png -quality 80 -resize &apos;280x140^&gt;&apos; -gravity Center -crop 280x140+0+0 +repage /Users/qiyun/Desktop/新图片.jpg</span><br></pre></td></tr></table></figure></p>
<p><code>-quality   图片质量,jpg默认99,png默认75
-resize
100x100      高度和宽度比例保留最高值，高比不变
100x100^     高度和宽度比例保留最低值，宽高比不变
100x100!     宽度和高度强制转换，忽视宽高比
100x100&gt;     更改长宽，当图片长或宽超过规定的尺寸
100x100&lt;     更改长宽 只有当图片长宽都超过规定的尺寸
100x100^&gt;    更改长宽，当图片长或宽超过规定的尺寸。高度和宽度比例保留最低值
100x100^&lt;    更改长宽，只有当图片长宽都超过规定的尺寸。高度和宽度比例保留最低值
100          按指定的宽度缩放，保持宽高比例
x100         按指定高度缩放，保持宽高比
-gravity NorthWest, North, NorthEast, West, Center, East,  SouthWest, South, SouthEast截取用的定位指令,定位截取区域在图片中的方位
-crop 200x200+0+0 截取用的截取指令 ,在用定位指令后,按后两位的偏移值偏移截取范围左上角的像素后,再按前两位的数值,从左上角开始截取相应大小的图片
+repage         去掉图片裁减后的空白
-dissolve 30    设定组合图片透明度dissolve示例
+/-profile *    去掉/添加图片exif信息</code></p>
<p><strong>把原始图片分割成多张小图</strong></p>
<pre><code>•convert src.jpg -crop 100x100 dest.jpg
</code></pre><p>假设src.jpg的大小是300x200,执行命令后将得到名为dest-0.jpg、dest-1.jpg…dest-5.jpg的6张大小为100x100的小图片。注意如果尺寸不是目标图片的整数倍，那么右边缘和下边缘的一部分图片就用实际尺寸</p>
<p><strong>在原始图片上剪裁一张指定尺寸的小图</strong></p>
<pre><code>•convert src.jpg -crop 100x80+50+30 dest.jpg
</code></pre><p>在原始图片的上距离上部30像素左部50为起点的位置,分别向右向下截取一块大小为100x80的图片。如果x相对于坐标，宽度不够100，那就取实际值。</p>
<pre><code>•convert src.jpg -gravity center -crop 100x80+0+0 dest.jpg
</code></pre><p>在原始图上截取中心部分一块100x80的图片</p>
<pre><code>•convert src.jpg -gravity southeast -crop 100x80+10+5 dest.jpg
</code></pre><p>在原始图上截取右下角距离下边缘10个像素，右边缘5个像素一块100x80的图片</p>
<p><strong>图片进行反色处理</strong></p>
<pre><code>•convert -negate src.jpg negate.jpg
</code></pre><p>##示例</p>
<p><strong>转换rgb为灰度图</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">计算公式 &gt;&gt; Grey = (R*38 + G*75 + B*15)&gt;&gt; 7</span><br><span class="line">convert /Users/qiyun/Desktop/屏幕快照\ 2018-02-23\ 下午2.51.29.png -set colorspace Gray -separate -average 1111.jpeg</span><br></pre></td></tr></table></figure>
<p><strong>-crop参数是从一个图片截取一个指定区域的子图片</strong><br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">格式如下:convert -crop widthxheight&#123;+-&#125;x&#123;+-&#125;y&#123;%&#125;</span><br></pre></td></tr></table></figure></p>
<p> <em>width 子图片宽度</em> <em>height 子图片高度</em><br> x 为正数时为从区域左上角的x坐标,为负数时,左上角坐标为0,然后从截出的子图片右边减去x象素宽度. y 为正数时为从区域左上角的y坐标,为负数时,左上角坐标为0,然后从截出的子图片上边减去y象素高度.  如convert -crop 300x400+10+10 src.jpg dest.jpg 从src.jpg坐标为x:10 y:10截取300x400的图片存为dest.jpg convert -crop 300x400-10+10 src.jpg dest.jpg 从src.jpg坐标为x:0 y:10截取290x400的图片存为dest.jpg</p>
<p>##总结</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Qiyun2014/2019/02/15/机器学习之开源项目/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qiyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="祁云的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/15/机器学习之开源项目/" itemprop="url">机器学习开源项目</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-02-15T15:08:48+08:00">
                2019-02-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>##开源项目</p>
<p>###开源框架</p>
<ul>
<li><p>OpenCV [GitHub - opencv/opencv: Open Source Computer Vision Library] (<a href="https://github.com/opencv/opencv">https://github.com/opencv/opencv</a>)</p>
</li>
<li><p>dlib [GitHub - davisking/dlib: A toolkit for making real world machine learning and data analysis applications in C++] (<a href="https://github.com/davisking/dlib">https://github.com/davisking/dlib</a>)</p>
</li>
<li><p>[Scikit-Learn] (<a href="https://github.com/scikit-learn/scikit-learn">https://github.com/scikit-learn/scikit-learn</a>)<br> Scikit-Learn是用于机器学习的Python模块，它建立在SciPy之上。</p>
</li>
<li><p>[Caffe] (<a href="https://github.com/BVLC/caffe">https://github.com/BVLC/caffe</a>)<br> Caffe是由神经网络中的表达式、速度、模块化产生的深度学习框架。</p>
</li>
<li><p>[Mxnet] (<a href="https://github.com/apache/incubator-mxnet">https://github.com/apache/incubator-mxnet</a>)</p>
</li>
<li><p>[Tensorflow] (<a href="https://github.com/search?utf8=✓&amp;q=tensorflow">https://github.com/search?utf8=✓&amp;q=tensorflow</a>)<br> TensorFlow是谷歌发布的第二代机器学习系统。</p>
</li>
<li><p>[PredictionIO] (<a href="https://github.com/PredictionIO/PredictionIO">https://github.com/PredictionIO/PredictionIO</a>)<br> PredictionIO是面向开发人员和数据科学家的开源机器学习服务器。它支持事件采集、算法调度、评估，以及经由REST APIs的预测结果查询。</p>
</li>
<li><p>[Keras] (<a href="https://github.com/fchollet/keras">https://github.com/fchollet/keras</a>)<br> Keras是极其精简并高度模块化的神经网络库，在TensorFlow或Theano上都能够运行，是一个高度模块化的神经网络库，支持GPU和CPU运算。</p>
</li>
</ul>
<p>###视觉检测和追踪</p>
<ul>
<li>RCNN，[Fast-RCNN，Faster-RCNN 现在最主流的检测框架，GitHub - rbgirshick/py-faster-rcnn: Faster R-CNN (Python implementation) – see <a href="https://github.com/ShaoqingRen/faster_rcnn">https://github.com/ShaoqingRen/faster_rcnn</a> for the official MATLAB version] (<a href="https://github.com/rbgirshick/py-faster-rcnn">https://github.com/rbgirshick/py-faster-rcnn</a>)</li>
<li>SSD，达到实时的检测算法，[GitHub - weiliu89/caffe at ssd] (<a href="https://github.com/weiliu89/caffe/tree/ssd">https://github.com/weiliu89/caffe/tree/ssd</a>)</li>
<li>DPM，早期的检测算法，使用 latent SVM [GitHub - rbgirshick/voc-dpm: Object detection system using deformable part models (DPMs) and latent SVM (voc-release5). You may want to use the latest tarball on my website. The github code may include code changes that have not been tested as thoroughly and will not necessarily reproduce the results on the website.] (<a href="https://github.com/rbgirshick/voc-dpm">https://github.com/rbgirshick/voc-dpm</a>)</li>
<li>TLD，非常鲁棒的跟踪算法 [GitHub - zk00006/OpenTLD: Official source code for TLD] (<a href="https://github.com/zk00006/OpenTLD">https://github.com/zk00006/OpenTLD</a>)</li>
</ul>
<p>###2018二月份最火的项目</p>
<ol>
<li><p>[FastPhotoStyle] (<a href="https://github.com/NVIDIA/FastPhotoStyle)（快速转换照片风格）">https://github.com/NVIDIA/FastPhotoStyle)（快速转换照片风格）</a></p>
<p>  FastPhotoStyle 这个项目是英伟达（NVIDIA）开发的一个 Python 库。该模型将内容照片和风格照片作为输入。然后它将风格照片的风格转移到内容照片，即就是将内容照片的风格转换为我们输入的风格照片的风格。</p>
<p>  开发人员举出了两个算法示例。首先，第一个是非常简单的迭代算法，只需要下载一张内容图和风格图，重新调整他们尺寸，然后运行图像风格化代码。第二个样例中，需要使用语义标签映射来创建程式化图像。</p>
<p>  <img src="https://ws2.sinaimg.cn/large/006tNc79gy1fqs4qi9mw8j30pj0oz0zy.jpg" alt></p>
</li>
</ol>
<ol start="2">
<li><p>[Twitter Scraper] (<a href="https://github.com/kennethreitz/twitter-scraper)（Twitter">https://github.com/kennethreitz/twitter-scraper)（Twitter</a> 爬虫）</p>
<p> 如果你在 Twitter 上发过文章，就知道 Twitter 自身的 API 有流速限制，当然，作为国内用户大多数人都没用使用过 Twitter ，这个 Python 库就是考虑到这一点，它没有 API 限流（不需要任何身份验证），也没有限制，并且速度非常快。开发人员可以用这个库爬取任何用户的任意一条 Twitter （推文）。<br> 而且，这个项目可以用于制作马尔科夫链，但是目前它只能适用于 Python 3.6 及以上版本。</p>
</li>
<li><p>[Handwriting Synthesis] (<a href="https://github.com/sjvasquez/handwriting-synthesis)（手写体合成）">https://github.com/sjvasquez/handwriting-synthesis)（手写体合成）</a></p>
<p> 这个项目来自亚历克斯 · 格雷夫斯（Alex Graves）撰写的论文（Generating Sequences with Recurrent Neural Networks）《用 RNN 生成序列》，正如存储库的名称所示，您可以生成不同风格的手写，是其中手写体合成实验的实现，它可以生成不同风格的手写字迹。模型包括初始化和偏置两个部分，其中初始化控制样例的风格，偏置控制样例的整洁度。作者在 GitHub 页面上呈现的样本的多样性真的很吸引人。他正在寻找贡献者来加强存储库，所以如果您有兴趣，可以研究去看看。</p>
<p> <img src="https://ws4.sinaimg.cn/large/006tNc79gy1fqs4qv0lcqj30gq0acab2.jpg" alt></p>
</li>
<li><p>[ENAS PyTorch] (<a href="https://github.com/carpedm20/ENAS-pytorch)（高效神经网络结构搜索）">https://github.com/carpedm20/ENAS-pytorch)（高效神经网络结构搜索）</a></p>
<p> 该项目是对论文《参数共享的高效神经网络结构搜索（Efficient Neural Architecture Search (ENAS) via Parameters Sharing）》的实现。ENAS 做什么？高效神经网络结构搜索，即 ENAS 减少了计算需求，将 NAS 的 GPU 计算时间减少了 1000 倍。他们通过共享大型计算图中的子图模型之间的参数共享来完成此操作。<br> 如何使用它的过程已经在 GitHub 页面上得到了很好的展示。实现这个库的先决条件是：<br> •    Python 3.6+<br> •    需要 PyTorch<br> •    tqdm，imageio，graphviz，tqdm，tensorboardX</p>
<p> <img src="https://ws1.sinaimg.cn/large/006tNc79gy1fqs4r7to4qj30lt08mabq.jpg" alt><br> <img src="https://ws3.sinaimg.cn/large/006tNc79gy1fqs4rikm1sj30qx08ejtg.jpg" alt><br> <img src="https://ws2.sinaimg.cn/large/006tNc79gy1fqs4rttg37g30jy0q5npg.gif" alt="-w360"> <img src="https://ws2.sinaimg.cn/large/006tNc79gy1fqs4sfpdr7g30jl0nfkjq.gif" alt="-w360"></p>
</li>
<li><p><a href="https://github.com/EvilPort2/Sign-Language">Sign Language</a>（手势语言识别）</p>
<p> 这是一个相对简单但又十分吸引人的机器学习项目。在 Python 中使用卷积神经网络构造模型，可以识别手势并将其转换为机器上的文本。该项目存储库的作者用 Tensorflow 和 Keras 共同搭建了 CNN 模型，他特别详细地说明了他是怎么创建这个项目的，以及每一步是怎么进行的。<br> 是不是感觉非常不错的，如果你对机器学习感兴趣，可以去这些开源项目中去研究和学习一下。另外，通过这几个开源项目，我们又一次了解了 Python 的重要性，所以，对于想学编程的读者或者初入职场的计算机学生，可以往 Python 方向学习一下。<br><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fqs4t4h2rpj30e806gdg2.jpg" alt></p>
</li>
</ol>
<p>##shader编程</p>
<ul>
<li><a href="https://www.shadertoy.com" target="_blank" rel="noopener">Shadertoy</a>：视觉技术网站之一，shader 编程共享网站，里面很多很酷的例子。有一些已经被我移植到 opengl es 上了（详见：<a href="https://github.com/nekocode/murmur">nekocode/murmur · GitHub</a>）</li>
<li><a href="https://www.openprocessing.org" target="_blank" rel="noopener">OpenProcessing</a>：图像处理，可视化的话 也不要忘记了 Processing ，这个站点上很多很酷的，甚至可交互的例子，有点 java 底子，又想研究可视化技术的话，它是一个十分经典而且不错的选择</li>
<li><a href="http://transitions.glsl.io" target="_blank" rel="noopener">GLSL.io – Open Collection of GLSL Transitions</a>：也是一个 glsl shader 分享网站，主要关注于图片切换效果上</li>
</ul>
<h2 id="图像缩放"><a href="#图像缩放" class="headerlink" title="图像缩放"></a><a href="https://en.wikipedia.org/wiki/Image_scaling" target="_blank" rel="noopener">图像缩放</a></h2><ul>
<li><a href="https://github.com/nagadomi/waifu2x">Github: nagadomi/waifu2x · GitHub</a></li>
</ul>
<p>使用卷积神经网络(Convolutional Neural Network, CNN)针对漫画风格的图片进行放大.<br>效果还是相当不错的, 下面是官方的Demo图:</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fqs4tiviv0j30cr0r47e4.jpg" alt></p>
<p>##车牌识别<br>EasyPR 开源的中文车牌识别项目,Opencv 实现代码清晰易读。<br><a href="https://github.com/liuruoze/EasyPR">GitHub - liuruoze/EasyPR: An open source project for chinese plate recognition. It aims to be Easy, Flexible, and Accurate. Welcome to contribute your expertise !</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Qiyun2014/2019/02/15/第三方音乐软件播放导致直播中断/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qiyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="祁云的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/15/第三方音乐软件播放导致直播中断/" itemprop="url">第三方音乐软件播放导致直播中断</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-02-15T15:08:48+08:00">
                2019-02-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h2><p><img src="https://ws4.sinaimg.cn/large/006tNbRwgy1fxdgblht5ej30go0tn47f.jpg" alt="-w240"> <img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fxdgbn3mqkj30go0tnh2h.jpg" alt="-w240"> <img src="https://ws4.sinaimg.cn/large/006tNbRwgy1fxdgbr7vc7j30go0tnn65.jpg" alt="-w240"></p>
<p>音频开播（最左）<br>退入到后台后<br>打开qq音乐并选择音乐进行播放（中间）<br>退入后台<br>回到斗鱼app（最右）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">开播时，观众可以正常听到主播的声音。</span><br><span class="line">开启qq音乐后，观众端一直处于“鲨鱼娘努力加载中...”的场景，并无法恢复直播。</span><br></pre></td></tr></table></figure>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>麦克风（播放和录制）、耳机（插入和拔出）都会通知Device进行指令接收，进行扬声器播放或者耳机音频数据传输。</p>
<p>可以使用音频会话向系统传达打算如何在应用中使用音频。<br>此音频会话充当应用和操作系统之间的中介 - 进而是底层音频硬件。<br>可以使用它来向操作系统传达应用程序音频的性质，而无需详细说明特定行为或与音频硬件的所需交互。将这些细节的管理委派给音频会话可确保最佳地管理用户的音频体验。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fxdgtafs5ij30pf0j3ac5.jpg" alt="-w600"></p>
<pre><code>•    Configure the audio session category and mode to communicate to the system how you intend to use audio in your app
•    Activate your app’s audio session to put your category and mode configuration into action
•    Subscribe and respond to important audio session notifications, such as audio interruptions and route changes
•    Perform advanced audio device configuration such as setting sample rate, I/O buffer duration, and number of channels
</code></pre><h4 id="打断场景"><a href="#打断场景" class="headerlink" title="打断场景"></a>打断场景</h4><p><img src="https://ws1.sinaimg.cn/large/006tNbRwgy1fxdhwy63emj30tz0hqq50.jpg" alt="-w600"></p>
<p>当直播库录制时，会创建一个session用于向系统传达指令。这个session在系统级的Device的session之上。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">开启直播，就会开启session；</span><br><span class="line"></span><br><span class="line">第三方音乐应用开启，会创建自己的session，抢占当前我们直播的session，导致直播音频录制停止。</span><br><span class="line"></span><br><span class="line">Facetime或者来电等请求接收时，设备的session会下发通知，当前有更底层的session需要使用，我们直播的session会被打断，导致直播音频录制停止。如果用户主动拒绝这些请求，打断会恢复，直播继续。</span><br></pre></td></tr></table></figure>
<p>==出现无法直播的的问题就是第三方音乐app抢占了session，直播直接停止==</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fxdi8iegqhj315v080tat.jpg" alt></p>
<p>iOS系统支持后台模式以及播放和录制情形共同使用的场景</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. 为了保证音频录制能在iOS系统后台持续工作，首先需要开启UIBackgroundModes配置，保证应用在静音、锁屏下正常使用。</span><br><span class="line">2. 监听当前Session的状态，是否打断、恢复</span><br><span class="line">3. 监听当前应用Active，第一时间响应app进入前后台的工作场景</span><br><span class="line">4. 回到前台时，重新启动Session并且设置Category允许Record和Play</span><br><span class="line">5. reactivate当前的AudioSession</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">AVAudioSession *session = [AVAudioSession sharedInstance];</span><br><span class="line">[session setCategory:AVAudioSessionCategoryPlayAndRecord withOptions:AVAudioSessionCategoryOptionMixWithOthers | AVAudioSessionCategoryOptionAllowBluetooth error:nil];</span><br><span class="line">[session setActive:YES error:nil];</span><br></pre></td></tr></table></figure>
<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><p><a href="https://developer.apple.com/library/archive/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/HandlingAudioInterruptions/HandlingAudioInterruptions.html" target="_blank" rel="noopener">https://developer.apple.com/library/archive/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/HandlingAudioInterruptions/HandlingAudioInterruptions.html</a></p>
<p><a href="https://developer.apple.com/library/archive/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/AudioSessionCategoriesandModes/AudioSessionCategoriesandModes.html" target="_blank" rel="noopener">https://developer.apple.com/library/archive/documentation/Audio/Conceptual/AudioSessionProgrammingGuide/AudioSessionCategoriesandModes/AudioSessionCategoriesandModes.html</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Qiyun2014/2019/02/15/如何高效的使用硬编码进行数据压缩/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qiyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="祁云的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/15/如何高效的使用硬编码进行数据压缩/" itemprop="url">如何高效的使用硬编码进行推流</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-02-15T15:08:48+08:00">
                2019-02-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何高效的使用硬编码进行推流"><a href="#如何高效的使用硬编码进行推流" class="headerlink" title="如何高效的使用硬编码进行推流"></a>如何高效的使用硬编码进行推流</h1><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>从摄像头和麦克风获取的音视频数据（YUV、PCM）因原始数据体积较大，不适合直接在网络进行传输。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNbRwgy1fxurz839d2j30oi0hptbd.jpg" alt></p>
<p>在直播中实时传输音视频，需要将音视频流分解成小块数据包，而原始数据需要添加音视频同步信息保证接收端能还原出完整的音视频流。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">所以，获取的音视频数据需要分辨进行编码，目前编码比较高效的音视频方式分辨是h264和aac。</span><br><span class="line"></span><br><span class="line">编码后的数据，相比原始数据进行很大程度的压缩，将压缩的数据以一定格式进行封装，进行muxer后通过rtmp协议推送到服务端。</span><br></pre></td></tr></table></figure>
<h3 id="推流参数介绍"><a href="#推流参数介绍" class="headerlink" title="推流参数介绍"></a>推流参数介绍</h3><ul>
<li><p><strong>FPS</strong>：视频每秒包含多少帧，一帧就是一个静态的图像；</p>
</li>
<li><p><strong>I帧，B帧和P帧</strong>：I帧是靠尽可能去除图像空间冗余信息来压缩传输数据量的帧内编码图像； P帧是通过充分降低与图像序列中前面已编码帧的时间冗余信息来压缩传输数据量的编码图像，也叫预测帧； B帧是既考虑与源图像序列前面已编码帧，也顾及源图像序列后面已编码帧之间的时间冗余信息来压缩传输数据量的编码图像，也叫双向预测帧；一般地，I帧压缩效率最低，P帧较高，B帧最高。I帧是通过帧内预测编码的，在获取到I帧后能解码出相应的图像，而P帧和B帧需要依赖相邻的帧才能解码出图像；</p>
</li>
<li><p><strong>GOP</strong>：( <strong>Group of Pictures </strong>) 是一组连续的画面，由一张 I 帧和数张 B / P 帧组成，是视频图像编码器和解码器存取的基本单位，它的排列顺序将会一直重复到影像结束。</p>
</li>
<li><p><strong>GOP</strong>间隔：是指一个画面序列中相邻两个I帧的间隔时间。</p>
</li>
<li><p><strong>封装格式</strong>：音视频文件封装格式有<strong>avi</strong>、<strong>rmvb</strong>、<strong>flv</strong>、<strong>mkv</strong>、<strong>MP4</strong>等等，封装格式就是把音视频数据打包成一个文件的规范。一般会包含音视频编码类型，时间戳，采样率等信息，方便接收方解析出原始数据。</p>
</li>
<li><p><strong>视频码率</strong>：码率的单位为<strong>kbps</strong>，指每秒视频数据量，其值跟每秒视频帧数FPS，图像分辨率以及编码器压缩率有关。</p>
</li>
</ul>
<h2 id="硬编码"><a href="#硬编码" class="headerlink" title="硬编码"></a>硬编码</h2><p>为了保证高清画质的视频数据在网络中达到实时直播的效果，且满足蜂窝网络、wifi下的正常使用。</p>
<p>需要对图像进行编码，也就是压缩。</p>
<p>常用的压缩算法有<strong>h264</strong>、<strong>HEVC</strong>、<strong>VP8</strong>、<strong>VP9</strong>等等。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fxus21trxtj30oa0gsacv.jpg" alt></p>
<p><strong>以iOS为例，目前仅支持H264硬编码，ffmpeg实现软编码（H264、H265)</strong></p>
<p><strong>在移动端使用，软编码占用CPU性能非常高，硬编码完全依赖GPU进行像素数据处理，速度会更快。</strong></p>
<h2 id="编码等级"><a href="#编码等级" class="headerlink" title="编码等级"></a>编码等级</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">H.264有四种画质级别，分别是BP、EP、MP、HP：</span><br></pre></td></tr></table></figure>
<p>在下面三种级别进行编码时，对应的耗电数据如下：</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fxuslp4kuxj30ob0ay404.jpg" alt></p>
<p><strong>高级别的画质，对数据保留完整度更高</strong></p>
<h3 id="CABAC和CAVLC比较"><a href="#CABAC和CAVLC比较" class="headerlink" title="CABAC和CAVLC比较"></a>CABAC和CAVLC比较</h3><p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fxuswsvt2bj30od0gpdjm.jpg" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">越高的编码等级编解码越复杂，压缩率越高，同样的网络速率能传输更多的视频信息；</span><br><span class="line"></span><br><span class="line">CABAC充分发挥算术编码压缩效率高的特点，而且其基于上下文的特点使其可以充分利用不同视频流的统计特性和符号间的相关性，自适应不同符号（消息）出现的概率。</span><br><span class="line"></span><br><span class="line">因此，CABAC的编码性能更好。</span><br></pre></td></tr></table></figure>
<p><strong>选择节能而码率相对较大的情况下，CAVLC明显更适合；<br>选择带宽节省的情况，CABAC无疑更好。</strong></p>
<p><strong>CABAC虽然性能很好，但也存在不足：</strong></p>
<ul>
<li>复杂度过高，不易并行处理。存在块及依赖（左/上角的块没有码率估计/熵编码, 后继块就无法得到更新后的状态，从而无法开始码率估计/上编码）、Bin级依赖（统一个子队列的bin存在前后依赖性，后继的Bin要等前面Bin编完后才能得到更新后的上下文状态）以及编码的几个环节依赖，这些依赖性会影响编码器的并行实现。</li>
<li>计算精度问题。为简化计算，CABAC采用128个状态来近似，根据原来状态和当前符号性质查表得到下个状态，这个过程会有一定程度的精度损失。</li>
<li>Context的利用频率过高或过低。</li>
</ul>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. 根据当前网络环境，设置对应的实时码率能有效的解决直播卡顿、带宽成本的问题。</span><br><span class="line">2. 不同的分辨率、帧率、GOP需要谨慎决策，能对直播带来良好的体验。</span><br><span class="line">3. CABAC和CAVLC的选择需要取舍（带宽还是功耗）</span><br><span class="line">4. HEVC相比VP9差别不大，但VP9谷歌开源，HEVC却有非常多的专利机构，使用成本较高。但两者相比H264，有显著程度的提高。</span><br><span class="line">5. HEVC迟早会替代H264。</span><br></pre></td></tr></table></figure>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://books.google.com/books?id=ysFeDwAAQBAJ&amp;pg=PT54&amp;lpg=PT54&amp;dq=cavlc+cabac比较&amp;source=bl&amp;ots=YIU0up78TW&amp;sig=AaFvh7527Oel2K-HaE5U1RUXeDs&amp;hl=zh-CN&amp;sa=X&amp;ved=2ahUKEwj2-6KloYXfAhVqx1QKHRQFACEQ6AEwCHoECAEQAQ#v=onepage&amp;q=cavlc%20cabac比较&amp;f=false" target="_blank" rel="noopener">网络电视技术</a></p>
<p><a href="https://www.jianshu.com/p/7450fd22ffc6" target="_blank" rel="noopener">算术编码 CABAC CAVLC</a></p>
<p><a href="http://www.lighterra.com/papers/videoencodingh264/" target="_blank" rel="noopener">H264视频编码设置</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Qiyun2014/2019/02/15/音频降噪算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qiyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="祁云的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/15/音频降噪算法/" itemprop="url">音频降噪算法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-02-15T15:08:48+08:00">
                2019-02-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="音频降噪算法"><a href="#音频降噪算法" class="headerlink" title="音频降噪算法"></a>音频降噪算法</h1><h2 id="WAV-Waveform-Audio-File-Format"><a href="#WAV-Waveform-Audio-File-Format" class="headerlink" title="WAV (Waveform Audio File Format)"></a>WAV (Waveform Audio File Format)</h2><p>此格式属于资源交换档案格式(RIFF)的应用之一，通常会将采用脉冲编码调制的音频资存储在区块中。<br>也是其音乐发烧友中常用的指定规格之一。<br>由于此音频格式未经过压缩，所以在音质方面不会出现失真的情况，但文件的体积因而在众多音频格式中较为大。</p>
<p><strong>制作WAV文件时，会有一个采样率，根据采样定理，若此采样率低于信号最高频率的两倍时，那么超过此最大频率的频段，将会产生混叠的情况，使原始信号受到污染，亦不能称此WAV文件就是无损文件。</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">WAV文件遵守资源交换档案格式之规则，在文件的前44(或46)字节放置标头(header)，使播放器或编辑器能够简单掌握文件的基本信息，其内容以区块(chunk)为最小单位，每一区块长度为4字节，而区块之上则由子区块包裹，每一子区块长度不拘，但须在前头先宣告标签及长度(字节)。</span><br><span class="line"></span><br><span class="line">标头的前3个区块记录文件格式及长度；</span><br><span class="line">接着第一个子区块包含8个区块，记录声道数量、采样率等信息；</span><br><span class="line">接着第二个子区块才是真正的音频数据，长度则视音频长度而定。</span><br><span class="line"></span><br><span class="line">内容如下表所示。须注意的是，每个区块的端序不尽相同，而音频内容本身则是采用小端序。</span><br></pre></td></tr></table></figure>
<p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fy61nb179gj30hx0drjto.jpg" alt></p>
<h2 id="AAC-Advanced-Audio-Coding"><a href="#AAC-Advanced-Audio-Coding" class="headerlink" title="AAC (Advanced Audio Coding)"></a>AAC (Advanced Audio Coding)</h2><p>出现于1997年，为一种基于MPEG-2的有损数字音频压缩的专利音频编码标准。</p>
<ul>
<li><p>.aac - 使用MPEG-2 Audio Transport Stream（ADTS，参见MPEG-2）容器，区别于使用MPEG-4容器的MP4/M4A格式，属于传统的AAC编码（FAAC默认的封装，但FAAC亦可输出MPEG-4封装的AAC）。</p>
</li>
<li><p>.mp4 - 使用了MPEG-4 Part 14（第14部分）的简化版即3GPP Media Release 6 Basic（3gp6，参见3GP）进行封装的AAC编码（Nero AAC编码器仅能输出MPEG-4封装的AAC）。</p>
</li>
<li><p>.m4a - 为了区别纯音频MP4文件和包含视频的MP4文件而由苹果（Apple）公司使用的扩展名，Apple iTunes对纯音频MP4文件采用了”.m4a”命名。M4A的本质和音频MP4相同，故音频MP4文件亦可直接更改扩展名为M4A。</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">作为一种高压缩比的音频压缩算法，AAC压缩比通常为18：1，也有数据说为20：1；</span><br><span class="line"></span><br><span class="line">在音质方面，由于采用多声道，和使用低复杂性的描述方式，使其比几乎所有的传统编码方式在同规格的情况下更胜一筹。</span><br><span class="line"></span><br><span class="line">不过直到2006年，使用这一格式存储音乐的并不多，可以播放该格式的mp3播放器更是少之又少，目前所知仅有苹果iPod、Sony Walkman（NWZ-A、NWZ-S、NWZ-E、NWZ-X系列）、任天堂NDSi和iPhone（微软推出的Windows 7附带的Windows Media Player 12也支持AAC）。</span><br><span class="line"></span><br><span class="line">此外计算机上很多音乐播放软件都支持AAC（前提是安装过AAC解码器），如苹果iTunes。</span><br><span class="line"></span><br><span class="line">但在移动电话领域，AAC的支持度已很普遍，Nokia、Sony Ericsson、Motorola等品牌均在其中高端产品中支持AAC（一开始主要是LC-AAC，随着移动电话性能的发展，HE-AAC的支持也已广泛）。</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">AAC是一种宽带音频编码算法，主要利用以下两种编码策略来大幅减少存储高质量数字音频所需要的数据量：</span><br><span class="line">舍去与感知上无关的信号成分</span><br><span class="line">去除编码后信号的冗余部分</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Qiyun2014/2019/02/15/绝地求生存活人数识别Tesseract-OCR/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qiyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="祁云的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/15/绝地求生存活人数识别Tesseract-OCR/" itemprop="url">Tesseract OCR使用介绍</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-02-15T15:08:48+08:00">
                2019-02-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>#Tesseract OCR使用介绍</p>
<p>##目录<br>[TOC]</p>
<p>##下载地址及介绍</p>
<ul>
<li>官网介绍：<a href="http://code.google.com/p/tesseract-ocr/wiki/TrainingTesseract3" target="_blank" rel="noopener">http://code.google.com/p/tesseract-ocr/wiki/TrainingTesseract3</a></li>
<li>Github源码连接： <a href="https://github.com/tesseract-ocr">https://github.com/tesseract-ocr</a></li>
<li>开源贡献者主页 <a href="https://kevintechnology.com/" target="_blank" rel="noopener">https://kevintechnology.com/</a></li>
</ul>
<p>##安装 Tesseract</p>
<ul>
<li>语言包查看 <a href="https://www.macports.org/ports.php?by=name&amp;substr=tesseract-" target="_blank" rel="noopener">https://www.macports.org/ports.php?by=name&amp;substr=tesseract-</a></li>
<li>支持Windows、linux、macOS</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1、安装 tesseract和语言包</span><br><span class="line">sudo port install tesseract	</span><br><span class="line">sudo port install tesseract-&lt;langcode&gt;</span><br><span class="line"></span><br><span class="line">2、homebrew 安装</span><br><span class="line">brew install tesseract</span><br><span class="line">brew install --with-training-tools tesseract</span><br><span class="line"></span><br><span class="line">3、重新安装</span><br><span class="line">brew uninstall tesseract</span><br><span class="line">brew install --with-training-tools tesseract</span><br></pre></td></tr></table></figure>
<ul>
<li>Homebrew 是一个包管理器，如果没装的话，在终端执行</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</span><br></pre></td></tr></table></figure>
<p>##使用 Tesseract</p>
<ul>
<li>使用命令行进行图像识别</li>
<li>imagename 就是要识别的图片文件的名称，outputbase 就是识别结果输出文件的名称。</li>
<li>lang 就是要识别的语言代码，例如英语为 eng、简体中文为 chi_sim 等等。可以同时识别多种语言，使用 “+” 相连，例如 eng+chi_sim。缺省时识别英语。</li>
</ul>
<p>1、格式信息如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tesseract imagename outputbase [-l lang] [-psm pagesegmode] [configfile...]</span><br></pre></td></tr></table></figure></p>
<p>2、示例: 识别image图片并将结果保存在out.txt文件中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tesseract image.png out -l chi_sim</span><br><span class="line">tesseract image.png out -l chi_sim -psm 10</span><br></pre></td></tr></table></figure></p>
<p>3、pagesegmode 为识别的具体模式，具体包含以下模式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">•	0 = Orientation and script detection (OSD) only.</span><br><span class="line">•	1 = Automatic page segmentation with OSD.</span><br><span class="line">•	2 = Automatic page segmentation, but no OSD, or OCR</span><br><span class="line">•	3 = Fully automatic page segmentation, but no OSD. (Default)</span><br><span class="line">•	4 = Assume a single column of text of variable sizes.</span><br><span class="line">•	5 = Assume a single uniform block of vertically aligned text.</span><br><span class="line">•	6 = Assume a single uniform block of text.</span><br><span class="line">•	7 = Treat the image as a single text line.</span><br><span class="line">•	8 = Treat the image as a single word.</span><br><span class="line">•	9 = Treat the image as a single word in a circle.</span><br><span class="line">•	10 = Treat the image as a single character.</span><br><span class="line">•	11 = Sparse text. Find as much text as possible in no particular order.</span><br><span class="line">•	12 = Sparse text with OSD.</span><br><span class="line">•	13 = Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific.</span><br></pre></td></tr></table></figure></p>
<p>##训练样本</p>
<ul>
<li>训练工具 <a href="https://github.com/tesseract-ocr/tesseract/wiki/AddOns">https://github.com/tesseract-ocr/tesseract/wiki/AddOns</a></li>
<li>使用教程 <a href="https://github.com/tesseract-ocr/tesseract/wiki/Training-Tesseract">https://github.com/tesseract-ocr/tesseract/wiki/Training-Tesseract</a></li>
<li>提高识别率 <a href="https://github.com/tesseract-ocr/tesseract/wiki/ImproveQuality">https://github.com/tesseract-ocr/tesseract/wiki/ImproveQuality</a></li>
<li>清理文本背景 <a href="http://www.fmwconcepts.com/imagemagick/textcleaner/index.php" target="_blank" rel="noopener">http://www.fmwconcepts.com/imagemagick/textcleaner/index.php</a></li>
<li>提取文本区域 <a href="http://www.danvk.org/2015/01/07/finding-blocks-of-text-in-an-image-using-python-opencv-and-numpy.html" target="_blank" rel="noopener">http://www.danvk.org/2015/01/07/finding-blocks-of-text-in-an-image-using-python-opencv-and-numpy.html</a></li>
<li>以jTessBoxEditor为例</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; 1、收集文本信息的图片</span><br><span class="line">&gt; 2、制作的图片转为tiff格式</span><br><span class="line">&gt; 3、jTessBoxEditor进行tiff格式图片合成 &lt;Tool-&gt;Merge TIFF&gt;</span><br><span class="line"></span><br><span class="line">合成后的图片取名规范 [lang].[fontname].exp[num].tif</span><br><span class="line">[lang]是语言，[fontname]是字体，[num]是标号</span><br></pre></td></tr></table></figure>
<p>###1、Make Box Files</p>
<ul>
<li>使用 Tesseract 识别，生成 box 文件：</li>
<li>确保 tif 和 box 文件同名且位于同一目录下，用 jTessBoxEditor 打开 tif 文件），或者直接用文本编辑器编辑。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tesseract hz.font.exp0.tif hz.font.exp0 -l chi_sim -psm 10 batch.nochop makebox</span><br></pre></td></tr></table></figure>
<p>###2、Run Tesseract for Training</p>
<ul>
<li>使用修改正确后的 box 文件，对 Tesseract 进行训练，生成 .tr 文件：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tesseract hz.font.exp0.tif hz.font.exp0 -psm 10 nobatch box.train</span><br></pre></td></tr></table></figure>
<p>###3、Compute the Character Set</p>
<ul>
<li>生成字符集的文本</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">unicharset_extractor hz.font.exp0.box hz.font.exp1.box</span><br><span class="line"></span><br><span class="line">After 3.03</span><br><span class="line">training/set_unicharset_properties -U input_unicharset -O output_unicharset --script_dir=training/langdata</span><br></pre></td></tr></table></figure>
<p>正确的格式应该如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">110</span><br><span class="line">NULL 0 NULL 0</span><br><span class="line">N 5 59,68,216,255,87,236,0,27,104,227 Latin 11 0 1 N</span><br><span class="line">Y 5 59,68,216,255,91,205,0,47,91,223 Latin 33 0 2 Y</span><br><span class="line">1 8 59,69,203,255,45,128,0,66,74,173 Common 3 2 3 1</span><br><span class="line">9 8 18,66,203,255,89,156,0,39,104,173 Common 4 2 4 9</span><br><span class="line">a 3 58,65,186,198,85,164,0,26,97,185 Latin 56 0 5 a</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>###4、font_properties (new in 3.01)</p>
<ul>
<li>定义字体特征文件，Tesseract-OCR 3.01 以上的版本在训练之前需要创建一个名称为 font_properties 的字体特征文件。font_properties 不含有 BOM 头，文件内容格式如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;fontname&gt; &lt;italic&gt; &lt;bold&gt; &lt;fixed&gt; &lt;serif&gt; &lt;fraktur&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>其中 fontname 为字体名称，必须与 [lang].[fontname].exp[num].box 中的名称保持一致。<italic> 、<bold> 、<fixed> 、<serif>、<fraktur> 的取值为 1 或 0，表示字体是否具有这些属性。</fraktur></serif></fixed></bold></italic></li>
<li>这里就是普通字体，不倾斜不加粗，所以新建一个名为 font_properties 的文件，内容为： font 0 0 0 0 0</li>
</ul>
<p>###5、Clustering</p>
<ul>
<li>修改 Clustering 过程生成的 4 个文件（inttemp、pffmtable、normproto、shapetable）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">shapeclustering -F font_properties -U unicharset hz.font.exp0.tr hz.font.exp1.tr ...</span><br><span class="line"></span><br><span class="line">mftraining -F font_properties -U unicharset -O hz.unicharset hz.font.exp0.tr hz.font.exp1.tr ...</span><br><span class="line"></span><br><span class="line">cntraining hz.font.exp0.tr hz.font.exp1.tr ...</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">* 生成后的文件需要添加前缀， 如这里改为 hz.inttemp、hz.pffmtable、hz.normproto、hz.shapetable。</span><br><span class="line"></span><br><span class="line">###6、Putting it all together</span><br><span class="line"></span><br><span class="line">* 生成最后的训练文件</span><br></pre></td></tr></table></figure>
<p>combine_tessdata hz.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">###7、use example</span><br><span class="line"></span><br><span class="line">* 使用训练的文件进行识别</span><br></pre></td></tr></table></figure></p>
<p>tesseract test.png out -l hz<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">##脚本运行</span><br></pre></td></tr></table></figure></p>
<p>#!/bin/sh<br>read -p “输入你语言:” lang<br>echo ${lang}<br>read -p “输入你的字体:” font<br>echo ${font}<br>echo “完整文件名为：”<br>echo ${lang}.${font}.exp0.tif<br>echo “开始。。。”<br>echo ${font} 0 0 0 0 0 &gt;font_properties</p>
<p>#tesseract  ${lang}.${font}.exp0.tif $(lang).$(font).exp0 -l chi_sim -psm 10 batch.nochop makebox</p>
<p>#read -p “继续生产tr文件？”<br>tesseract  ${lang}.${font}.exp0.tif ${lang}.${font}.exp0 -psm 10 nobatch box.train<br>unicharset_extractor ${lang}.${font}.exp0.box<br>shapeclustering -F font_properties -U unicharset ${lang}.${font}.exp0.tr<br>mftraining -F font_properties -U unicharset -O unicharset ${lang}.${font}.exp0.tr<br>cntraining ${lang}.${font}.exp0.tr<br>echo “开始重命名文件”<br>mv inttemp ${font}.inttemp<br>mv normproto ${font}.normproto<br>mv pffmtable ${font}.pffmtable<br>mv shapetable ${font}.shapetable<br>mv unicharset ${font}.unicharset<br>echo “生成最终文件”<br>combine_tessdata ${font}.<br>echo “完成”<br><code>`</code></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Qiyun2014/2019/02/15/神经网路介绍/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qiyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="祁云的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/15/神经网路介绍/" itemprop="url">深度神经网络</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-02-15T15:08:48+08:00">
                2019-02-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>#深度神经网络</p>
<h2 id="TOC"><a href="#TOC" class="headerlink" title="TOC"></a>TOC</h2><p>[TOC]</p>
<p><strong>介绍</strong></p>
<p>2016 年 3 月，谷歌公司的 AlphaGo 向韩国棋院围棋九段大师李世石发起挑战，而这棋局走法的可能性有 361!种，最终 AlphaGo 战 胜了这场“棋局数比可见宇宙中的原子数还多”的智力游戏。2015 年 11 月 9 日(在距这场比 赛前4个月)，谷歌公司开源了它的第二代深度学习系统TensorFlow，也就是AlphaGo的基础 程序。 </p>
<p>2017 年 2 月，TensorFlow 的首届开发者峰会(2017 TensorFlow Dev Summit)在美国的 加利福尼亚州举行。在会上，谷歌公司宣布正式发布 TensorFlow 1.0 版本.</p>
<p>众所周知，人工智能是高级计算智能最宽泛的概念，机器学习是研究人工智能的一个工 具，深度学习是机器学习的一个子集，是目前研究领域卓有成效的学习方法。深度学习的框 架有很多，而 TensorFlow 将神经网络、算法这些平时停留在理论层面的知识，组织成一个平 台框架，集合了神经网络的各个算法函数组成一个工具箱，让广大工程师可以专心建造自己 的目标领域的“轮子”，而且 TensorFlow 是基于 Python 语言的，极易上手，这些优势迅速吸 引了全世界的工程师。 </p>
<p>##深度学习 </p>
<p>###深度<br>深度学习的前身是人工神经网络(artificial neural network，ANN)，它的基本特点就是试图 模仿人脑的神经元之间传递和处理信息的模式。神经网络这个词本身可以指生物神经网络和人 工神经网络。在机器学习中，我们说的神经网络一般就是指人工神经网络。</p>
<p>下图给出的是一个最基本的人工神经网络的 3 层模型。 </p>
<p>前馈神经网络示意图 </p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fqr8gk2gs7j307206zmxt.jpg" alt></p>
<p>###神经网络</p>
<p><strong>前馈神经网络</strong><br><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fqr8h9k4jhj31en0mfgo1.jpg" alt></p>
<ul>
<li><p>神经网络其实是一个非常宽泛的称呼，它包括两类，一类是用计算机的方式去模拟人脑，这就是我们常说的ANN（人工神经网络），另一类是研究生物学上的神经网络，又叫生物神经网络。对于我们计算机人士而言，肯定是研究前者。</p>
</li>
<li><p>在人工神经网络之中，又分为前馈神经网络和反馈神经网络这两种。那么它们两者的区别是什么呢？这个其实在于它们的结构图。我们可以把结构图看作是一个有向图。其中神经元代表顶点，连接代表有向边。对于前馈神经网络中，这个有向图是没有回路的。你可以仔细观察本文中出现的所有神经网络的结构图，确认一下。而对于反馈神经网络中，结构图的有向图是有回路的。反馈神经网络也是一类重要的神经网络。其中Hopfield网络就是反馈神经网络。深度学习中的RNN也属于一种反馈神经网络。</p>
</li>
<li><p>本文中所分别描述的三个网络：单层神经网络，双层神经网络，以及多层神经网络。深度学习中的CNN属于一种特殊的多层神经网络。另外，在一些Blog中和文献中看到的BP神经网络是什么？其实它们就是使用了反向传播BP算法的两层前馈神经网络。也是最普遍的一种两层神经网络。</p>
</li>
</ul>
<p>神经网络是一种模拟人脑的神经网络以期能够实现类人工智能的机器学习技术。人脑中的神经网络是一个非常复杂的组织。成人的大脑中估计有1000亿个神经元之多。</p>
<ul>
<li>它包括输入层（input layer）、输出层（output layer）和一个或多个隐藏层（hidden layers）。上图的神经网络由3个单元的输入层，4个单元的隐藏层和2个单元的输出层组成。单元等于感知器。</li>
<li>输入层的单元是隐藏层单元的输入，隐藏层单元的输出是输出层单元的输入。</li>
<li>两个感知器之间的连接有一个权量。</li>
<li>第t层的每个感知器与第t-1层的每个感知器相互关联。当然，你也可以设置权量为0，从而在实质上取消连接。</li>
<li>在加工输入数据时，你将输入数据赋予输入层的每个单元，而隐藏层的每个单元是输入层每个单元的加权求和。也就是说，输入层的数据会被前向传播到隐藏层的每个单元。同理，隐藏层的输出作为输入会前向传播到输入层，计算得到最后的输出，即神经网络的输出。</li>
<li>多个隐藏层的神经网络同理。</li>
</ul>
<p>###神经元</p>
<ul>
<li>对于神经元的研究由来已久，1904年生物学家就已经知晓了神经元的组成结构。</li>
<li>一个神经元通常具有多个树突，主要用来接受传入信息；而轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“突触”。</li>
</ul>
<p>人脑中的神经元形状可以用下图做简单的说明：<br><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fqr8hrvi0xj30d807ugpk.jpg" alt></p>
<ul>
<li>神经元模型是一个包含输入，输出与计算功能的模型。输入可以类比为神经元的树突，而输出可以类比为神经元的轴突，计算则可以类比为细胞核。</li>
<li>下图是一个典型的神经元模型：包含有3个输入，1个输出，以及2个计算功能。</li>
<li>注意中间的箭头线。这些线称为“连接”。每个上有一个“权值”。</li>
</ul>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fqr8i4zslcj30gu09qq4i.jpg" alt></p>
<ul>
<li>连接是神经元中最重要的东西。每一个连接上都有一个权重。</li>
<li>一个神经网络的训练算法就是让权重的值调整到最佳，以使得整个网络的预测效果最好。</li>
<li>我们使用a来表示输入，用w来表示权值。一个表示连接的有向箭头可以这样理解：在初端，传递的信号大小仍然是a，端中间有加权参数w，经过这个加权后的信号会变成a<em>w，因此在连接的末端，信号的大小就变成了a</em>w。</li>
<li>在其他绘图模型里，有向箭头可能表示的是值的不变传递。而在神经元模型里，每个有向箭头表示的是值的加权传递。</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fqr8io5cinj30ca06z750.jpg" alt></p>
<ul>
<li>如果我们将神经元图中的所有变量用符号表示，并且写出输出的计算公式的话，就是下图。</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fqr8j3kkboj30hd099gmp.jpg" alt></p>
<ul>
<li>可见z是在输入和权值的线性加权和叠加了一个函数g的值。在MP模型里，函数g是sgn函数，也就是取符号函数。这个函数当输入大于0时，输出1，否则输出0。</li>
<li>下面对神经元模型的图进行一些扩展。首先将sum函数与sgn函数合并到一个圆圈里，代表神经元的内部计算。其次，把输入a与输出z写到连接线的左上方，便于后面画复杂的网络。最后说明，一个神经元可以引出多个代表输出的有向箭头，但值都是一样的。</li>
<li>神经元可以看作一个计算与存储单元。计算是神经元对其的输入进行计算功能。存储是神经元会暂存计算结果，并传递到下一层</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fqr8jk38saj30ma08rac1.jpg" alt></p>
<ul>
<li>当我们用“神经元”组成网络以后，描述网络中的某个“神经元”时，我们更多地会用“单元”（unit）来指代。同时由于神经网络的表现形式是一个有向图，有时也会用“节点”（node）来表达同样的意思。 </li>
</ul>
<p><strong>神经元模型的使用可以这样理解:</strong></p>
<ul>
<li>我们有一个数据，称之为样本。样本有四个属性，其中三个属性已知，一个属性未知。我们需要做的就是通过三个已知属性预测未知属性。</li>
<li>具体办法就是使用神经元的公式进行计算。三个已知属性的值是a1，a2，a3，未知属性的值是z。z可以通过公式计算出来。</li>
<li>这里，已知的属性称之为特征，未知的属性称之为目标。假设特征与目标之间确实是线性关系，并且我们已经得到表示这个关系的权值w1，w2，w3。那么，我们就可以通过神经元模型预测新样本的目</li>
</ul>
<p>###两层神经网络</p>
<ul>
<li>单层神经网络无法解决异或问题。但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。不过两层神经网络的计算是一个问题，没有一个较好的解法</li>
<li>两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。我们扩展上节的单层神经网络，在右边新加一个层次（只含有一个节点）。</li>
</ul>
<p><strong>例如ax(y)代表第y层的第x个节点。z1，z2变成了a1(2)，a2(2)。下图给出了a1(2)，a2(2)的计算公式。</strong></p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fqr8jthf9gj30fx0au77c.jpg" alt></p>
<p><strong>计算最终输出z的方式是利用了中间层的a1(2)，a2(2)和第二个权值矩阵计算得到的，如下图。</strong></p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fqr8k7oq83j30g60at76v.jpg" alt></p>
<p><strong>与单层神经网络不同。理论证明，两层神经网络可以无限逼近任意连续函数。</strong><br><strong>这是什么意思呢？也就是说，面对复杂的非线性分类任务，两层（带一个隐藏层）神经网络可以分类的很好</strong></p>
<ul>
<li>两层神经网络通过两层的线性模型模拟了数据内真实的非线性函数。因此，多层的神经网络的本质就是复杂函数拟合。</li>
<li>在设计一个神经网络时，输入层的节点数需要与特征的维度匹配，输出层的节点数要与目标的维度匹配。而中间层的节点数，却是由设计者指定的。因此，“自由”把握在设计者的手中。但是，节点数设置的多少，却会影响到整个模型的效果。如何决定这个自由层的节点数呢？目前业界没有完善的理论来指导这个决策。一般是根据经验来设置。较好的方法就是预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的值作为最终选择。这种方法又叫做Grid Search（网格搜索）</li>
</ul>
<p>如EasyPR字符识别网络架构（下图）。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fqr8kevp4pj30fa0atgnx.jpg" alt></p>
<p>EasyPR使用了字符的图像去进行字符文字的识别。输入是120维的向量。输出是要预测的文字类别，共有65类。根据实验，我们测试了一些隐藏层数目，发现当值为40时，整个网络在测试集上的效果较好，因此选择网络的最终结构就是120，40，65。</p>
<p>###模型</p>
<p><code>1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络。他给它起了一个名字--“感知器”（Perceptron）（有的文献翻译成“感知机”）。</code><br><code>感知器是当时首个可以学习的人工神经网络。Rosenblatt现场演示了其学习识别简单图像的过程，在当时的社会引起了轰动。</code></p>
<p>####loss</p>
<ul>
<li>Rosenblat提出的感知器模型中，模型中的参数可以被训练，但是使用的方法较为简单，并没有使用目前机器学习中通用的方法，这导致其扩展性与适用性非常有限。从两层神经网络开始，神经网络的研究人员开始使用机器学习相关的技术进行神经网络的训练。例如用大量的数据（1000-10000左右），使用算法进行优化等等，从而使得模型训练可以获得性能与数据利用上的双重优势。</li>
<li>机器学习模型训练的目的，就是使得参数尽可能的与真实的模型逼近。具体做法是这样的。首先给所有参数赋上随机值。我们使用这些随机生成的参数值，来预测训练数据中的样本。样本的预测目标为yp，真实目标为y。那么，定义一个值loss，计算公式如下。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = (yp - y)2</span><br></pre></td></tr></table></figure>
<p>==这个值称之为损失（loss），我们的目标就是使对所有训练数据的损失和尽可能的小。==<br><strong>如果将先前的神经网络预测的矩阵公式带入到yp中（因为有z=yp），那么我们可以把损失写为关于参数（parameter）的函数，这个函数称之为损失函数（loss function）。下面的问题就是求：如何优化参数，能够让损失函数的值最小。</strong></p>
<p>此时这个问题就被转化为一个优化问题。一个常用方法就是高等数学中的求导，但是这里的问题由于参数不止一个，求导后计算导数等于0的运算量很大，所以一般来说解决这个优化问题使用的是梯度下降算法。梯度下降算法每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。</p>
<p>在神经网络模型中，由于结构复杂，每次计算梯度的代价很大。因此还需要使用反向传播算法。反向传播算法是利用了神经网络的结构进行的计算。不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。</p>
<p>####反向传播算法<br>==反向传播算法可以直观的理解为下图。梯度的计算从后往前，一层层反向传播。前缀E代表着相对导数的意思。==</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fqr8kpxij9j30b80aqtak.jpg" alt></p>
<p><strong>反向传播算法的启示是数学中的链式法则。在此需要说明的是，尽管早期神经网络的研究人员努力从生物学中得到启发，但从BP算法开始，研究者们更多地从数学上寻求问题的最优解。不再盲目模拟人脑网络是神经网络研究走向成熟的标志。正如科学家们可以从鸟类的飞行中得到启发，但没有必要一定要完全模拟鸟类的飞行方式，也能制造可以飞天的飞机。</strong></p>
<p>####正则化<br><strong>优化问题只是训练中的一个部分。机器学习问题之所以称为学习问题，而不是优化问题，就是因为它不仅要求数据在训练集上求得一个较小的误差，在测试集上也要表现好。因为模型最终是要部署到没有见过训练数据的真实场景。提升模型在测试集上的预测效果的主题叫做泛化（generalization），相关方法被称作正则化（regularization）。神经网络中常用的泛化技术有权重衰减等。</strong></p>
<p>两层神经网络在多个地方的应用说明了其效用与价值。10年前困扰神经网络界的异或问题被轻松解决。神经网络在这个时候，已经可以发力于语音识别，图像识别，自动驾驶等多个领域。<br>但是神经网络仍然存在若干的问题：尽管使用了BP算法，一次神经网络的训练仍然耗时太久，而且困扰训练优化的一个问题就是局部最优解问题，这使得神经网络的优化较为困难。同时，隐藏层的节点数需要调参，这使得使用不太方便，工程和研究人员对此多有抱怨。</p>
<p>####SVM<br>90年代中期，由Vapnik等人发明的SVM（Support Vector Machines，支持向量机）算法诞生，很快就在若干个方面体现出了对比神经网络的优势：无需调参；高效；全局最优解。基于以上种种理由，SVM迅速打败了神经网络算法成为主流。</p>
<p>###多层神经网络（深度学习）</p>
<p><strong>2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。与传统的训练方式不同，“深度信念网络”有一个“预训练”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词–“深度学习”。</strong></p>
<p><strong>很快，深度学习在语音识别领域暂露头角。接着，2012年，深度学习技术又在图像识别领域大展拳脚。Hinton与他的学生在ImageNet竞赛中，用多层的卷积神经网络成功地对包含一千类别的一百万张图片进行了训练，取得了分类错误率15%的好成绩，这个成绩比第二名高了近11个百分点，充分证明了多层神经网络识别效果的优越性。</strong></p>
<p><strong>在这之后，关于深度神经网络的研究与应用不断涌现。</strong></p>
<p>在两层神经网络的输出层后面，继续添加层次。原来的输出层变成中间层，新加的层次成为新的输出层。所以可以得到下图。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fqr8kyx0b0j30f509pwgy.jpg" alt></p>
<p>依照这样的方式不断添加，我们可以得到更多层的多层神经网络。公式推导的话其实跟两层神经网络类似，使用矩阵运算的话就仅仅是加一个公式而已。<br>==在已知输入a(1)，参数W(1)，W(2)，W(3)的情况下，输出z的推导公式如下：==<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">g(W(1) * a(1)) = a(2); </span><br><span class="line">     g(W(2) * a(2)) = a(3);</span><br><span class="line">g(W(3) * a(3)) = z;</span><br></pre></td></tr></table></figure></p>
<p>多层神经网络中，输出也是按照一层一层的方式来计算。从最外面的层开始，算出所有单元的值以后，再继续计算更深一层。只有当前层所有单元的值都计算完毕以后，才会算下一层。有点像计算向前不断推进的感觉。所以这个过程叫做“正向传播”。</p>
<p>==首先看第一张图，可以看出W(1)中有6个参数，W(2)中有4个参数，W(3)中有6个参数，所以整个神经网络中的参数有16个（这里我们不考虑偏置节点，下同）。==</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fqr8m1g5q1j30gf0cvdiu.jpg" alt></p>
<p>==假设我们将中间层的节点数做一下调整。第一个中间层改为3个单元，第二个中间层改为4个单元。经过调整以后，整个网络的参数变成了33个。==</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fqr8m91njxj30hf0dqtd4.jpg" alt></p>
<p>==虽然层数保持不变，但是第二个神经网络的参数数量却是第一个神经网络的接近两倍之多，从而带来了更好的表示（represention）能力。表示能力是多层神经网络的一个重要性质。<br>在参数一致的情况下，我们也可以获得一个“更深”的网络==</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fqr8mlqpt1j30iw0chtdc.jpg" alt></p>
<p>==上图的网络中，虽然参数数量仍然是33，但却有4个中间层，是原来层数的接近两倍。这意味着一样的参数数量，可以用更深的层次去表达。==</p>
<p><strong>与两层层神经网络不同。多层神经网络中的层数增加了很多。<br>增加更多的层次有什么好处？更深入的表示特征，以及更强的函数模拟能力。 更深入的表示特征可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。</strong></p>
<p><strong>多层神经网络特征学习如图：</strong></p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fqr8mw3nuwj30m70c9tch.jpg" alt></p>
<ul>
<li>更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的容量（capcity）去拟合真正的关系。</li>
<li>通过研究发现，在参数数量一样的情况下，更深的网络往往具有比浅层的网络更好的识别效率。这点也在ImageNet的多次大赛中得到了证实。从2012年起，每年获得ImageNet冠军的深度神经网络的层数逐年增加，2015年最好的方法GoogleNet是一个多达22层的神经网络。</li>
<li>在最新一届的ImageNet大赛上，目前拿到最好成绩的MSRA团队的方法使用的更是一个深达152层的网络！关于这个方法更多的信息有兴趣的可以查阅ImageNet网站。</li>
</ul>
<p>###训练</p>
<ul>
<li>在单层神经网络时，我们使用的激活函数是sgn函数。到了两层神经网络时，我们使用的最多的是sigmoid函数。而到了多层神经网络时，通过一系列的研究发现，ReLU函数在训练多层神经网络时，更容易收敛，并且预测性能更好。因此，目前在深度学习中，最流行的非线性函数是ReLU函数。ReLU函数不是传统的非线性函数，而是分段线性函数。其表达式非常简单，就是y=max(x,0)。简而言之，在x大于0，输出就是输入，而在x小于0时，输出就保持为0。这种函数的设计启发来自于生物神经元对于激励的线性响应，以及当低于某个阈值后就不再响应的模拟。</li>
<li>在多层神经网络中，训练的主题仍然是优化和泛化。当使用足够强的计算芯片（例如GPU图形加速卡）时，梯度下降算法以及反向传播算法在多层神经网络中的训练中仍然工作的很好。目前学术界主要的研究既在于开发新的算法，也在于对这两个算法进行不断的优化，例如，增加了一种带动量因子（momentum）的梯度下降算法。　</li>
<li>在深度学习中，泛化技术变的比以往更加的重要。这主要是因为神经网络的层数增加了，参数也增加了，表示能力大幅度增强，很容易出现过拟合现象。因此正则化技术就显得十分重要。目前，Dropout技术，以及数据扩容（Data-Augmentation）技术是目前使用的最多的正则化技术</li>
</ul>
<p>##神经网络发展<br>多层神经网络的研究仍在进行中。现在最为火热的研究技术包括RNN，LSTM等，研究方向则是图像理解方面。图像理解技术是给计算机一幅图片，让它用语言来表达这幅图片的意思。ImageNet竞赛也在不断召开，有更多的方法涌现出来，刷新以往的正确率。</p>
<p>神经网络的发展历史曲折荡漾，既有被人捧上天的时刻，也有摔落在街头无人问津的时段，中间经历了数次大起大落。<br>从单层神经网络（感知器）开始，到包含一个隐藏层的两层神经网络，再到多层的深度神经网络，一共有三次兴起过程。详见下图。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fqr8n3bkcvj30m90eiwi8.jpg" alt></p>
<ul>
<li><p>上图中的顶点与谷底可以看作神经网络发展的高峰与低谷。图中的横轴是时间，以年为单位。纵轴是一个神经网络影响力的示意表示。如果把1949年Hebb模型提出到1958年的感知机诞生这个10年视为落下（没有兴起）的话，那么神经网络算是经历了“三起三落”这样一个过程，跟“小平”同志类似。俗话说，天将降大任于斯人也，必先苦其心志，劳其筋骨。经历过如此多波折的神经网络能够在现阶段取得成功也可以被看做是磨砺的积累吧。</p>
</li>
<li><p>历史最大的好处是可以给现在做参考。科学的研究呈现螺旋形上升的过程，不可能一帆风顺。同时，这也给现在过分热衷深度学习与人工智能的人敲响警钟，因为这不是第一次人们因为神经网络而疯狂了。1958年到1969年，以及1985年到1995，这两个十年间人们对于神经网络以及人工智能的期待并不现在低，可结果如何大家也能看的很清楚。</p>
</li>
<li><p>因此，冷静才是对待目前深度学习热潮的最好办法。如果因为深度学习火热，或者可以有“钱景”就一窝蜂的涌入，那么最终的受害人只能是自己。神经网络界已经两次有被人们捧上天了的境况，相信也对于捧得越高，摔得越惨这句话深有体会。因此，神经网络界的学者也必须给这股热潮浇上一盆水，不要让媒体以及投资家们过分的高看这门技术。很有可能，三十年河东，三十年河西，在几年后，神经网络就再次陷入谷底。根据上图的历史曲线图，这是很有可能的。</p>
</li>
</ul>
<p>###表示能力<br>==下面说一下神经网络为什么能这么火热？简而言之，就是其学习效果的强大。随着神经网络的发展，其表示性能越来越强。==<br>==从单层神经网络，到两层神经网络，再到多层神经网络，下图说明了，随着网络层数的增加，以及激活函数的调整，神经网络所能拟合的决策分界平面的能力。==</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fqr8nae3loj31go0yzdjx.jpg" alt></p>
<p>可以看出，随着层数增加，其非线性分界拟合能力不断增强。图中的分界线并不代表真实训练出的效果，更多的是示意效果。<br>神经网络的研究与应用之所以能够不断地火热发展下去，与其强大的函数拟合能力是分不开关系的。</p>
<p>当然，光有强大的内在能力，并不一定能成功。一个成功的技术与方法，不仅需要内因的作用，还需要时势与环境的配合。神经网络的发展背后的外在原因可以被总结为：更强的计算性能，更多的数据，以及更好的训练方法。只有满足这些条件时，神经网络的函数拟合能力才能得已体现，见下图。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fqr8nid4z3j31010ml76c.jpg" alt></p>
<ul>
<li><p>之所以在单层神经网络年代，Rosenblat无法制作一个双层分类器，就在于当时的计算性能不足，Minsky也以此来打压神经网络。但是Minsky没有料到，仅仅10年以后，计算机CPU的快速发展已经使得我们可以做两层神经网络的训练，并且还有快速的学习算法BP。</p>
</li>
<li><p>但是在两层神经网络快速流行的年代。更高层的神经网络由于计算性能的问题，以及一些计算方法的问题，其优势无法得到体现。直到2012年，研究人员发现，用于高性能计算的图形加速卡（GPU）可以极佳地匹配神经网络训练所需要的要求：高并行性，高存储，没有太多的控制需求，配合预训练等算法，神经网络才得以大放光彩。</p>
</li>
<li><p>互联网时代，大量的数据被收集整理，更好的训练方法不断被发现。所有这一切都满足了多层神经网络发挥能力的条件。</p>
</li>
</ul>
<p>###展望</p>
<p>####量子计算</p>
<ul>
<li><p>回到我们对神经网络历史的讨论，根据历史趋势图来看，神经网络以及深度学习会不会像以往一样再次陷入谷底？作者认为，这个过程可能取决于量子计算机的发展。</p>
</li>
<li><p>根据一些最近的研究发现，人脑内部进行的计算可能是类似于量子计算形态的东西。而且目前已知的最大神经网络跟人脑的神经元数量相比，仍然显得非常小，仅不及1%左右。所以未来真正想实现人脑神经网络的模拟，可能需要借助量子计算的强大计算能力。</p>
</li>
<li><p>各大研究组也已经认识到了量子计算的重要性。谷歌就在开展量子计算机D-wave的研究，希望用量子计算来进行机器学习，并且在前段时间有了突破性的进展。国内方面，阿里和中科院合作成立了量子计算实验室，意图进行量子计算的研究。</p>
</li>
<li><p>如果量子计算发展不力，仍然需要数十年才能使我们的计算能力得以突飞猛进的发展，那么缺少了强大计算能力的神经网络可能会无法一帆风顺的发展下去。这种情况可以类比为80-90年时期神经网络因为计算能力的限制而被低估与忽视。假设量子计算机真的能够与神经网络结合，并且助力真正的人工智能技术的诞生，而且量子计算机发展需要10年的话，那么神经网络可能还有10年的发展期。直到那时期以后，神经网络才能真正接近实现AI这一目标。</p>
</li>
</ul>
<p>####人工智能</p>
<ul>
<li><p>虽然现在人工智能非常火热，但是距离真正的人工智能还有很大的距离。就拿计算机视觉方向来说，面对稍微复杂一些的场景，以及易于混淆的图像，计算机就可能难以识别。因此，这个方向还有很多的工作要做。就普通人看来，这么辛苦的做各种实验，以及投入大量的人力就是为了实现一些不及孩童能力的视觉能力，未免有些不值。但是这只是第一步。虽然计算机需要很大的运算量才能完成一个普通人简单能完成的识图工作，但计算机最大的优势在于并行化与批量推广能力。使用计算机以后，我们可以很轻易地将以前需要人眼去判断的工作交给计算机做，而且几乎没有任何的推广成本。这就具有很大的价值。正如火车刚诞生的时候，有人嘲笑它又笨又重，速度还没有马快。但是很快规模化推广的火车就替代了马车的使用。人工智能也是如此。这也是为什么目前世界上各著名公司以及政府都对此热衷的原因。</p>
</li>
<li><p>目前看来，神经网络要想实现人工智能还有很多的路要走，但方向至少是正确的，下面就要看后来者的不断努力了。</p>
</li>
</ul>
<p>##卷积神经网络</p>
<p>###视觉感知</p>
<p><strong>一、画面识别是什么任务？</strong></p>
<p>学习知识的第一步就是明确任务，清楚该知识的输入输出。卷积神经网络最初是服务于画面识别的，所以我们先来看看画面识别的实质是什么。</p>
<p>==先观看几组动物与人类视觉的差异对比图。==</p>
<ol>
<li>苍蝇的视觉和人的视觉的差异</li>
</ol>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fqr8ns85glj30k007hmy1.jpg" alt></p>
<ol start="2">
<li>蛇的视觉和人的视觉的差异</li>
</ol>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fqr8nz6lwmj30k007ht94.jpg" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">通过上面的两组对比图可以知道，即便是相同的图片经过不同的视觉系统，也会得到不同的感知。</span><br><span class="line"></span><br><span class="line">这里引出一条知识：生物所看到的景象并非世界的原貌，而是长期进化出来的适合自己生存环境的一种感知方式。 蛇的猎物一般是夜间行动，所以它就进化出了一种可以在夜间也能很好观察的感知系统，感热。</span><br></pre></td></tr></table></figure>
<p>任何视觉系统都是将图像反光与脑中所看到的概念进行关联。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fqr8o8fkp0j306702p0sk.jpg" alt></p>
<p>所以画面识别实际上并非识别这个东西客观上是什么，而是寻找人类的视觉关联方式，并再次应用。 如果我们不是人类，而是蛇类，那么画面识别所寻找的𝒇就和现在的不一样。</p>
<p><code>画面识别实际上是寻找（学习）人类的视觉关联方式𝒇，并再次应用。</code></p>
<p>###图像表达</p>
<p>我们知道了“画面识别是从大量的数据中寻找人类的视觉关联方式𝒇，并再次应用。 其-是输入，表示所看到的东西-输出，表示该东西是什么。</p>
<p>在自然界中，是物体的反光，那么在计算机中，图像又是如何被表达和存储的呢？</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fqr8ofdicog3083082mxw.gif" alt></p>
<p>图像在计算机中是一堆按顺序排列的数字，数值为0到255。0表示最暗，255表示最亮。 你可以把这堆数字用一个长长的向量来表示，也就是==tensorflow==的mnist教程中784维向量的表示方式。 然而这样会失去平面结构的信息，为保留该结构信息，通常选择矩阵的表示方式：28x28的矩阵。</p>
<p>上图是只有黑白颜色的灰度图，而更普遍的图片表达方式是RGB颜色模型，即红（Red）、绿（Green）、蓝（Blue）三原色的色光以不同的比例相加，以产生多种多样的色光。<br>这样，RGB颜色模型中，单个矩阵就扩展成了有序排列的三个矩阵，也可以用三维张量去理解，其中的每一个矩阵又叫这个图片的一个channel。</p>
<p>在电脑中，一张图片是数字构成的“长方体”。可用 宽width, 高height, 深depth 来描述，如图。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fqr8p1pqfuj308h070jr9.jpg" alt></p>
<p><code>画面识别的输入是shape为(width, height, depth)的三维张量。</code></p>
<p>接下来要考虑的就是该如何处理这样的“数字长方体”。</p>
<p>####画面不变性</p>
<p>在决定如何处理“数字长方体”之前，需要清楚所建立的网络拥有什么样的特点。 我们知道一个物体不管在画面左侧还是右侧，都会被识别为同一物体，这一特点就是不变性（invariance），如下图所示。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fqr8p9azpkj30fr0ldmy8.jpg" alt></p>
<p>我们希望所建立的网络可以尽可能的满足这些不变性特点。<br>为了理解卷积神经网络对这些不变性特点的贡献，我们将用不具备这些不变性特点的前馈神经网络来进行比较。</p>
<p>####图片识别–前馈神经网络</p>
<p>方便起见，我们用depth只有1的灰度图来举例。 想要完成的任务是：在宽长为4x4的图片中识别是否有下图所示的“横折”。 图中，黄色圆点表示值为0的像素，深色圆点表示值为1的像素。 我们知道不管这个横折在图片中的什么位置，都会被认为是相同的横折。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fqr8phpoakj30bf0aeq3f.jpg" alt></p>
<p>若训练前馈神经网络来完成该任务，那么表达图像的三维张量将会被摊平成一个向量，作为网络的输入，即(width, height, depth)为(4, 4, 1)的图片会被展成维度为16的向量作为网络的输入层。再经过几层不同节点个数的隐藏层，最终输出两个节点，分别表示“有横折的概率”和“没有横折的概率”，如下图所示。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fqr8pp9m6gj30dv0e174o.jpg" alt></p>
<p>下面我们用数字（16进制）对图片中的每一个像素点（pixel）进行编号。 当使用右侧那种物体位于中间的训练数据来训练网络时，网络就只会对编号为5,6,9,a的节点的权重进行调节。 若让该网络识别位于右下角的“横折”时，则无法识别。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fqr8pvt89zj30k00cejs1.jpg" alt></p>
<p>解决办法是用大量物体位于不同位置的数据训练，同时增加网络的隐藏层个数从而扩大网络学习这些变体的能力。<br>然而这样做十分不效率，因为我们知道在左侧的“横折”也好，还是在右侧的“横折”也罢，大家都是“横折”。 为什么相同的东西在位置变了之后要重新学习？有没有什么方法可以将中间所学到的规律也运用在其他的位置？ 换句话说，也就是让==不同位置用相同的权重==。</p>
<p>==卷积神经网络就是让权重在不同位置共享的神经网络。==</p>
<p>####局部连接</p>
<ul>
<li><p>在卷积神经网络中，我们先选择一个局部区域，用这个局部区域去扫描整张图片。 局部区域所圈起来的所有节点会被连接到下一层的一个节点上。</p>
</li>
<li><p>为了更好的和前馈神经网络做比较，我将这些以矩阵排列的节点展成了向量。 下图展示了被红色方框所圈中编号为0,1,4,5的节点是如何通过连接到下一层的节点0上的。</p>
</li>
</ul>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fqr8q2zrfqj30e90eagm6.jpg" alt></p>
<p>这个带有连接强弱的红色方框就叫做 filter 或 kernel 或 feature detector。 而filter的范围叫做filter size，这里所展示的是2x2的filter size。<br> <img src="https://ws2.sinaimg.cn/large/006tNc79gy1fqr8q9syjaj302f01i0sl.jpg" alt></p>
<p>第二层的节点0的数值就是局部区域的线性组合，即被圈中节点的数值乘以对应的权重后相加。 用表示输入值，表示输出值，用图中标注数字表示角标，则下面列出了两种计算编号为0的输出值的表达式。</p>
<p>注：在局部区域的线性组合后，也会和前馈神经网络一样，加上一个偏移量（偏移量充当阈值）。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fqr8qh1u9zj309v0360ss.jpg" alt></p>
<p>####空间共享</p>
<p>当filter扫到其他位置计算输出节点时，包括是共用的。</p>
<p>下面这张动态图展示了当filter扫过不同区域时，节点的链接方式。 动态图的最后一帧则显示了所有连接。 可以注意到，每个输出节点并非像前馈神经网络中那样与全部的输入节点连接，而是部分连接。 这也就是为什么大家也叫前馈神经网络（feedforward neural network）为fully-connected neural network。 </p>
<p>图中显示的是一步一步的移动filter来扫描全图，一次移动多少叫做stride。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fqr8qonn5pg30e90eaq3x.gif" alt></p>
<p><code>空间共享也就是卷积神经网络所引入的先验知识。</code></p>
<p>####输出表达</p>
<p>如先前在图像表达中提到的，图片不用向量去表示是为了保留图片平面结构的信息。 同样的，卷积后的输出若用上图的排列方式则丢失了平面结构信息。 所以我们依然用矩阵的方式排列它们，就得到了下图所展示的连接。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fqr8qxgpl4j30e90ea0tb.jpg" alt></p>
<p>这也就是你们在网上所看到的下面这张图。在看这张图的时候请结合上图的连接一起理解，即输入（绿色）的每九个节点连接到输出（粉红色）的一个节点上的</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fqr8r5s4qkg30em0aogn2.gif" alt></p>
<p>==经过一个feature detector计算后得到的粉红色区域也叫做一个“Convolved Feature” 或 “Activation Map” 或 “Feature Map”。==</p>
<p>####Depth维的处理</p>
<p>现在我们已经知道了depth维度只有1的灰度图是如何处理的。 但前文提过，图片的普遍表达方式是下图这样有3个channels的RGB颜色模型。 当depth为复数的时候，每个feature detector是如何卷积的？</p>
<p><strong>现象</strong>：2x2所表达的filter size中，一个2表示width维上的局部连接数，另一个2表示height维上的局部连接数，并却没有depth维上的局部连接数，是因为depth维上并非局部，而是全部连接的。</p>
<p>==在2D卷积中，filter在张量的width维, height维上是局部连接，在depth维上是贯串全部channels的。==</p>
<p><strong>类比</strong>：想象在切蛋糕的时候，不管这个蛋糕有多少层，通常大家都会一刀切到底，但是在长和宽这两个维上是局部切割。</p>
<p>下面这张图展示了，在depth为复数时，filter是如何连接输入节点到输出节点的。 图中红、绿、蓝颜色的节点表示3个channels。 黄色节点表示一个feature detector卷积后得到的Feature Map。 其中被透明黑框圈中的12个节点会被连接到黄黑色的节点上。</p>
<pre><code>•    在输入depth为1时：被filter size为2x2所圈中的4个输入节点连接到1个输出节点上。
•    在输入depth为3时：被filter size为2x2，但是贯串3个channels后，所圈中的12个输入节点连接到1个输出节点上。
•    在输入depth为时：2x2x个输入节点连接到1个输出节点上。
</code></pre><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fqr8rfgjwaj308e0am74h.jpg" alt></p>
<p><strong>注意</strong>：三个channels的权重并不共享。 即当深度变为3后，权重也跟着扩增到了三组，如式子(3)所示，不同channels用的是自己的权重。 式子中增加的角标r,g,b分别表示red channel, green channel, blue channel的权重。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fqr8rooz1nj308301jq2w.jpg" alt></p>
<p>计算例子：用表示red channel的编号为0的输入节点，表示green channel编号为5个输入节点。表示blue channel。如式子(4)所表达，这时的一个输出节点实际上是12个输入节点的线性组合。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fqr8rzdcy2j30jf02aaa9.jpg" alt></p>
<p>当filter扫到其他位置计算输出节点时，那12个权重在不同位置是共用的，如下面的动态图所展示。 透明黑框圈中的12个节点会连接到被白色边框选中的黄色节点上。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fqr8s6ulz0g30920cn4b9.gif" alt></p>
<p>####Zero padding<br>细心的读者应该早就注意到了，4x4的图片被2x2的filter卷积后变成了3x3的图片，每次卷积后都会小一圈的话，经过若干层后岂不是变的越来越小？ Zero padding就可以在这时帮助控制Feature Map的输出尺寸，同时避免了边缘信息被一步步舍弃的问题。<br>例如：下面4x4的图片在边缘Zero padding一圈后，再用3x3的filter卷积后，得到的Feature Map尺寸依然是4x4不变</p>
<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fqr8slkkt7j304t04tjrc.jpg" alt></p>
<p>通常大家都想要在卷积时保持图片的原始尺寸。 选择3x3的filter和1的zero padding，或5x5的filter和2的zero padding可以保持图片的原始尺寸。 这也是为什么大家多选择3x3和5x5的filter的原因。 另一个原因是3x3的filter考虑到了像素与其距离为1以内的所有其他像素的关系，而5x5则是考虑像素与其距离为2以内的所有其他像素的关系。</p>
<p>==尺寸：Feature Map的尺寸等于(input_size + 2 * padding_size − filter_size)/stride+1。==</p>
<p>注意：上面的式子是计算width或height一维的。padding_size也表示的是单边补零的个数。</p>
<p>==例如(4+2-3)/1+1 = 4，保持原尺寸。==</p>
<p>不用去背这个式子。其中(input_size + 2 * padding_size)是经过Zero padding扩充后真正要卷积的尺寸。 减去 filter_size后表示可以滑动的范围。 再除以可以一次滑动（stride）多少后得到滑动了多少次，也就意味着得到了多少个输出节点。 再加上第一个不需要滑动也存在的输出节点后就是最后的尺寸。</p>
<p>####形状、概念抓取</p>
<p>知道了每个filter在做什么之后，我们再来思考这样的一个filter会抓取到什么样的信息。<br>我们知道不同的形状都可由细小的“零件”组合而成的。比如下图中，用2x2的范围所形成的16种形状可以组合成格式各样的“更大”形状。</p>
<p>卷积的每个filter可以探测特定的形状。又由于Feature Map保持了抓取后的空间结构。若将探测到细小图形的Feature Map作为新的输入再次卷积后，则可以由此探测到“更大”的形状概念。 比如下图的第一个“大”形状可由2,3,4,5基础形状拼成。第二个可由2,4,5,6组成。第三个可由6,1组成。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fqr8su8ctvj30dg083t96.jpg" alt></p>
<p>除了基础形状之外，颜色、对比度等概念对画面的识别结果也有影响。卷积层也会根据需要去探测特定的概念。</p>
<p>可以从下面这张图中感受到不同数值的filters所卷积过后的Feature Map可以探测边缘，棱角，模糊，突出等概念。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fqr8t03gguj309i0fmwey.jpg" alt></p>
<p>如我们先前所提，图片被识别成什么不仅仅取决于图片本身，还取决于图片是如何被观察的。<br>而filter内的权重矩阵W是网络根据数据学习得到的，也就是说，我们让神经网络自己学习以什么样的方式去观察图片。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fqr8t9s3c8j305i0730sr.jpg" alt></p>
<p>拿老妇与少女的那幅图片举例，当标签是少女时，卷积网络就会学习抓取可以成少女的形状、概念。 当标签是老妇时，卷积网络就会学习抓取可以成老妇的形状、概念。<br>下图展现了在人脸识别中经过层层的卷积后，所能够探测的形状、概念也变得越来越抽象和复杂。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fqr8th20n0j30a60f6q3f.jpg" alt></p>
<p><code>卷积神经网络会尽可能寻找最能解释训练数据的抓取方式。</code></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Qiyun2014/2019/02/15/x264编码使用介绍/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qiyun">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="祁云的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/15/x264编码使用介绍/" itemprop="url">x264编码使用介绍</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-02-15T14:45:42+08:00">
                2019-02-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x264 is a free software library and application for encoding video streams into the H.264/MPEG-4 AVC compression format, and is released under the terms of the GNU GPL.</span><br></pre></td></tr></table></figure>
<p>x264是一个采用GPL授权的视频编码自由软件。x264的主要功能在于进行H.264/MPEG-4 AVC的视频编码，而不是作为解码器（decoder）之用。</p>
<h2 id="特点介绍"><a href="#特点介绍" class="headerlink" title="特点介绍"></a>特点介绍</h2><ul>
<li><p>提供一流的性能，压缩和功能。</p>
</li>
<li><p>实现卓越的性能，在单个消费级计算机上实时编码4个或更多1080p流。</p>
</li>
<li><p>提供最好的质量，拥有最先进的心理视觉优化。</p>
</li>
<li><p>支持许多不同应用所必需的功能，例如电视广播，蓝光低延迟视频应用和网络视频。</p>
</li>
<li><p>x264构成了许多网络视频服务的核心，例如Youtube，Facebook，Vimeo和Hulu。它被电视广播公司和互联网服务提供商广泛使用。</p>
</li>
</ul>
<h2 id="x264-下载安装"><a href="#x264-下载安装" class="headerlink" title="x264 下载安装"></a>x264 下载安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//Download Address</span><br><span class="line">http://www.videolan.org/developers/x264.html</span><br><span class="line"></span><br><span class="line">//Git clone</span><br><span class="line">git clone https://git.videolan.org/git/x264.git</span><br></pre></td></tr></table></figure>
<h2 id="软编码和硬编码比较"><a href="#软编码和硬编码比较" class="headerlink" title="软编码和硬编码比较"></a>软编码和硬编码比较</h2><p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fyl3efc9nfj30lo0hmq6i.jpg" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 软编码</span><br><span class="line">实现直接、简单，参数调整方便，升级易，但CPU负载重，性能较硬编码低，低码率下质量通常比硬编码要好一点。</span><br><span class="line"></span><br><span class="line">// 硬编码</span><br><span class="line">性能高，低码率下通常质量低于硬编码器，但部分产品在GPU硬件平台移植了优秀的软编码算法（如X264）的，质量基本等同于软编码。</span><br></pre></td></tr></table></figure>
<h2 id="xcode-功耗截图"><a href="#xcode-功耗截图" class="headerlink" title="xcode 功耗截图"></a>xcode 功耗截图</h2><h3 id="硬编码"><a href="#硬编码" class="headerlink" title="硬编码"></a>硬编码</h3><p><img src="https://ws2.sinaimg.cn/large/006tNbRwgy1fyl3qifnl8j30r80cg0us.jpg" alt></p>
<h3 id="软编码"><a href="#软编码" class="headerlink" title="软编码"></a>软编码</h3><p><img src="https://ws4.sinaimg.cn/large/006tNbRwgy1fyl3ppotwtj30sq0bkab2.jpg" alt></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">软编占用CPU较高，开启NEON后会使用多核CPU</span><br><span class="line"></span><br><span class="line">相比硬编GPU占用整体耗电非常高的情形，软编会分担部分压力</span><br><span class="line"></span><br><span class="line">功耗有明显降低</span><br><span class="line"></span><br><span class="line">高端机型如iPhonex编码长时间发热现象不明显</span><br></pre></td></tr></table></figure>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1g073yxoij6j30qo0mpdld.jpg" alt></p>
<h3 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h3><p><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1g073zvnavqj30n90o079g.jpg" alt></p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1g07402s2sqj30kp03wdgk.jpg" alt></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qiyun</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qiyun</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
